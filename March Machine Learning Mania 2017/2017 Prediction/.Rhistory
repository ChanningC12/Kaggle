boxplot(Sales~ShelveLoc)
85/400
set.seed(2)
dim(Carseats)
train = sample(1:nrow(Carseats),200)
Carseats.test = Carseats[-train,]
High.test = High[-train]
tree.carseats = tree(High~.-Sales,data = Carseats, subset = train)
tree.pred = predict(tree.carseats,Carseats.test,type = "class")
table(tree.pred,High.test)
cv.carseats = cv.tree(tree.carseats,FUN = prune.misclass)
set.seed(3)
cv.carseats = cv.tree(tree.carseats,FUN = prune.misclass)
names(cv.carseats)
cv.carseats
par(mfrow = c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
prune.carseats = prune.misclass(tree.carseats,best=10)
par(mfrow = c(1,1))
plot(prune.carseats)
text(prune.carseats,pretty=0,cex=0.6)
tree.pred = predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston),nrow(Boston)/2)
View(Boston)
str(Boston)
help("Boston")
train = sample(1:nrow(Boston),nrow(Boston)/2)
tree.boston = tree(medv~.,data=Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0,cex=0.7)
cv.boston = cv.tree(tree.boston)
cv.boston
plot(cv.boston$size,cv.boston$dev,type="b")
prune.boston = prune.tree(tree.boston,best=5)
plot(prune.boston)
text(prune.boston,pretty=0,cex=0.8)
yhat = predict(tree.boston,newdata=Boston[-train,])
boston.test = Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
prune.boston
library(randomForest)
set.seed(1)
dim(Boston)
library(randomForest)
bag.boston = randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=T)
bag.boston
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag - boston.test)^2)
# Random forest proceeds in exactly the same way, except that we use a smaller value of mtry
set.seed(1)
rf.boston = randomForest(medv~.,data=Boston,subset=train,mtry=6,importance = T)
yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
set.seed(1)
?gbm
??gbm
install.packages("gbm")
library("gbm", lib.loc="~/R/win-library/3.2")
boost.boston = gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4) # distribution="bernoulli" if it were a binary classification problem
summary(boost.boston)
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
yhat.boost = predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
boost.boston = gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.2,verbose=F)
yhat.boost = predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
View(credit)
library(ISLR)
View(Smarket)
str(Smarket)
dim(Smarket)
library(plyr)
count(Smarket$Year)
help(Smarket)
round(cor(Smarket[,-9]),2)
library(corrplot)
corrplot(cor(Smarket[,-9],use="pairwise.complete.obs"))
corrplot(cor(Smarket[,-9],use="pairwise.complete.obs"),order = "hclust",method = "shade", tl.cex=0.7)
plot(Volume)
attach(Smarket)
plot(Volume)
glm.fit = glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, family = binomial, data = Smarket)
summary(glm.fit)
coef(glm.fit)
summary(glm.fit$coef)
glm.probs = predict(glm.fit,type="response")
glm.probs[1:10]
glm.pred = rep("Down",1250)
glm.pred[glm.probs>0.5] = "Up"
table(glm.pred,Direction)
mean(glm.pred == Direction)
Smarket = Smarket
glm.fit = glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, family = binomial, data = Smarket)
Smarket$prob = predict(glm.fit, type = "response")
View(Smarket)
Smarket$Direction_pred = ifelse(Smarket$prob>0.5,"Up","Down")
table(Direction_pred,Direction)
table(Smarket$Direction_pred,Smarket$Direction)
mean(Smarket$Direction_pred,Smarket$Direction)
mean(Smarket$Direction_pred=Smarket$Direction)
mean(Smarket$Direction_pred==Smarket$Direction)
library(foreign)
library(nnet)
library(ggplot2)
library(reshape1)
library(reshape2)
ml <- read.dta("http://www.ats.ucla.edu/stat/data/hsbdemo.dta")
View(ml)
str(ml)
ml = as.data.frame(ml)
str(ml)
attach(ml)
table(ses,prog)
tapply(write, prog, mean)
ml$prog2 = relevel(ml$prog, ref="academic")
View(ml)
test = multinom(prog2~ses+write, data=ml)
summary(test)
z <- summary(test)$coefficients/summary(test)$standard.errors
z
p <- (1 - pnorm(abs(z), 0, 1))*2
p
exp(2)
exp(1)
exp(coef(test))
head(pp=fitted(test))
head(pp=fitted(test))
head(fitted(test))
dses <- data.frame(ses = c("low", "middle", "high"),
write = mean(ml$write))
View(dses)
predict(test, newdata = dses, "probs")
dwrite <- data.frame(ses = rep(c("low", "middle", "high"), each = 41),
write = rep(c(30:70), 3))
View(dwrite)
pp.write <- cbind(dwrite, predict(test, newdata = dwrite, type = "probs", se = TRUE))
by(pp.write[, 3:5], pp.write$ses, colMeans)
View(pp.write)
lpp <- melt(pp.write, id.vars = c("ses", "write"), value.name = "probability")
head(lpp) # view first few rows
View(lpp)
ggplot(lpp, aes(x = write, y = probability, colour = ses)) +
geom_line() +
facet_grid(variable ~ ., scales="free")
120755 + 189952
23506+24184
279356+23926
121466+189241
767.39+447.76
138875+54739
55528+138086
54739-55528
303635/310707
187519/195657
5764+12428
303282+353
216173/303282
194495+147
216173+152
216173-12458
194495-11236
12946/2\
12946/2
202130-195657
6473-1408
216325-202130
195657-189184
28390/2
174837-9958
14195-546
202130-195657
152+546+1408+13649+6437+9958+164879
188100-187519
89.5+7.7+2.82
194638-147
188100-582
17578+8747
26896-26325
138875/195657
2942/195657
195500-10835
138875+54739
138875+54739+2043
308664+2043
data.df = data.frame(topic=c(rep(c("Gossip","Sports","Weather"),each=4)),
duration = c(6:9, 2:5, 4:7))
View(data.df)
attach(data.df)
boxplot(duration~topic, data=data.df)
boxplot(duration~topic, data=data.df, ylab="Duration of conversation")
lm(duration~topic, data=data.df)
summary(lm(duration~topic, data=data.df))
model.lm = lm(duration~topic, data=data.df)
rsq = summary(model.lm)$r.squared
sqrt(rsq)
print(model.lm$fitted.values)
cor(data.df$duration, model.lm$fitted.values)
library(heplots)
install.packages("heplots")
library(heplots)
library(heplots)
model.aov = aov(duration~topic, data=data.df)
summary(model.aov)
etasq(model.aov,partial=F)
librar(MASS)
library(MASS)
View(survey)
help(survey)
tbl = table(survey$Smoke, survey$Exer)
tbl
chisq.test(tbl)
data(anxiety)
View(anxiety)
library(irr)
install.packages("irr")
library(irr)
library(irr)
data(anxiety)
View(anxiety)
help(anxiety)
apply(anxiety,2,table)
icc(anxiety,model="twoway",type="agreement")
r1 = round(rnorm(20,10,4))
r2 = round(r1+10+rnorm(20,0,2))
r3 = round(r1+20+rnorm(20,0,2))
icc(cbind(r1,r2,r3),"twoway")
install.packages("lsr")
library(lsr)
library(lsr)
cramersV(tbl)
chisq.test(tbl)
tbl
26402-22688
392*4
4.8/2.7
4.82/2.68
86.9/91.3
37500-25000
?prop.test
prop.test(c(882,27791),c(7774,290343)
prop.test(c(882,27791),c(7774,290343))
prop.test(c(882,7774),c(27791,290343))
16512+24282+52313+82200
32715+19687+73507+14236+78788+30738+40672
1210/0.79
1210/0.8
1520/40
1210+68
1278/1600
1278/1800
1210/0.8
1210/1520
2080*75%
2080*0.75
520-310
210/40
150*26
3900*.25
data("mtcars")
mtcars = mtcars
sapply(mtcars,function(x) sum(is.na(x)))
a = sapply(mtcars,function(x) sum(is.na(x)))
a = data.frame(a)
View(a)
a$pct = a$a/nrow(mtcars)
View(a)
data()
a = data("AirPassengers")
a = AirPassengers
a = CO2
sapply(,function(x) sum(is.na(x)))
sapply(CO2,function(x) sum(is.na(x)))
sapply(Nile,function(x) sum(is.na(x)))
sapply(Orange,function(x) sum(is.na(x)))
sapply(women,function(x) sum(is.na(x)))
sapply(trees,function(x) sum(is.na(x)))
sapply(Swiss,function(x) sum(is.na(x)))
87323+203020
interaction = read.csv("../Desktop/deloitte_sample_interactions.csv")
View(interaction)
sample = read.csv("../Desktop/deloitte_sample.csv")
View(sample)
length(unqiue(sample[c("pfz_cust_id","brand")]))
length(unique(sample[c("pfz_cust_id","brand")]))
unique(sample[c("pfz_cust_id","brand")])
nrow(unique(sample[c("pfz_cust_id","brand")]))
table(sample$year)
View(interaction)
length(unique(interaction$pfz_cust_id))
length(unique(interaction$interaction_id))
names(interaction)
filter(interaction,n(interaction_id)>1)
library(dplyr)
nrow(unique(sample['pfz_cust_id','brand']))
nrow(unique(sample[c("pfz_cust_id","brand")]))
interaction_sub = interaction %>%
group_by(interaction_id) %>%
mutate(Len = n()) %>%
filter(Len>1)
View(interaction_sub)
?duplicated
interaction[duplicated(interaction),]
table(interaction_sub$Len)
interaction_sub_5 = interaction_sub %>%
filter(Len>5)
View(interaction_sub_5)
nrow(unique(interaction[c("interaction_id","intractn_sts_grouped")]))
nrow(unique(interaction[c("interaction_id","intractn_sts_grouped","std_brnd_num")]))
nrow(unique(interaction[c("interaction_id","intractn_sts_grouped","std_brnd_nm")]))
nrow(unique(interaction[c("interaction_id","intractn_sts_grouped","pfz_cust_id")]))
write.csv(interaction_sub_5,"../Desktop/interaction_sub_5.csv")
nrow(unique(interaction[c("interaction_id","intractn_sts_grouped","email_seq_num")]))
table(interaction$intractn_type_cd)
REPEMAIL = interaction %>%
filter(intractn_type_cd=="REPEAIL")
REPEMAIL = interaction %>%
filter(intractn_type_cd=="REPEMAIL")
View(REPEMAIL)
write.csv(REPEMAIL,"../Desktop/REPEMAIL.csv")
nrow(unique(interaction[c("interaction_id","intractn_sts_grouped","intractn_tmstmp")])) # unique by
nrow(unique(interaction[c("interaction_id","intractn_sts_grouped","email_seq_num")])) # unique by
nrow(unique(interaction[c("interaction_id","email_seq_num")])) # unique by
library(MASS)
library(blockcluster)
coclus_data<-read.csv("../Desktop/coclus_data_cleansed.csv",na.strings=c("NA","NaN",""))
coclus_data<-read.csv("../Desktop/Genworth/Genworth_Code/R Code/coclus_data_cleansed.csv",na.strings=c("NA","NaN",""))
str(coclus_data)
View(coclus_data)
data(binarydata)
View(binarydata)
class(binarydata)
class(coclus_data)
coclus_data_sub <- lapply(coclus_data[,2:ncol(coclus_data)], function(x) as.numeric(as.character(x)))
coclus_data_sub = as.data.frame(coclus_data_sub)
str(coclus_data_sub)
coclus_data = data.frame(Resource=coclus_data[,1],coclus_data_sub)
class(coclus_data)
str(coclus_data)
coclus_data[is.na(coclus_data)]=0
colSums(is.na(coclus_data))
y <- as.matrix(coclus_data)
class(y)
dim(y)
str(binarydata)
class(binarydata)
coclus_model = cocluster(y, datatype = 'continuous', nbcocluster=c(2,2))
data(gaussiandata)
View(gaussiandata)
?coclusterStrategy
dim(gaussiandata)
newstrategy<-cocluststrategy(nbxem=5,nbtry=2,algo="XEMStrategy")
library(blockcluster)
newstrategy<-cocluststrategy(nbxem=5,nbtry=2,algo="XEMStrategy")
newstrategy<-coclusterStrategy(nbxem=5,nbtry=2,algo="XEMStrategy")
out<-cocluster(gaussiandata,datatype="continuous",nbcocluster=c(2,3),strategy=newstrategy)
newstrategy<-coclusterStrategy(nbxem=5,nbtry=2)
out<-cocluster(gaussiandata,datatype="continuous",nbcocluster=c(2,3),strategy=newstrategy)
summary(out)
plot(out)
View(gaussiandata)
set.seed(1680) # for reproducibility
library(dplyr) # for data cleaning
library(ISLR) # for college dataset
library(cluster) # for gower similarity and pam
library(Rtsne) # for t-SNE plot
library(ggplot2) # for visualization
install.packages("Rtsne")
library(dplyr) # for data cleaning
library(ISLR) # for college dataset
library(cluster) # for gower similarity and pam
library(Rtsne) # for t-SNE plot
library(ggplot2) # for visualization
data("College")
data(College)
college_clean <- College %>%
mutate(name = row.names(.),
accept_rate = Accept/Apps,
isElite = cut(Top10perc,
breaks = c(0, 50, 100),
labels = c("Not Elite", "Elite"),
include.lowest = TRUE)) %>%
mutate(isElite = factor(isElite)) %>%
select(name, accept_rate, Outstate, Enroll,
Grad.Rate, Private, isElite)
View(college_clean)
glimpse(college_clean)
gower_dist <- daisy(college_clean[, -1],
metric = "gower",
type = list(logratio = 3))
summary(gower_dist)
gower_mat <- as.matrix(gower_dist)
View(gower_mat)
dim(gower_mat)
college_clean[
which(gower_mat == min(gower_mat[gower_mat != min(gower_mat)]),
arr.ind = TRUE)[1, ], ]
college_clean[
which(gower_mat == max(gower_mat[gower_mat != max(gower_mat)]),
arr.ind = TRUE)[1, ], ]
sil_width <- c(NA)
for(i in 2:10){
pam_fit <- pam(gower_dist,
diss = TRUE,
k = i)
sil_width[i] <- pam_fit$silinfo$avg.width
}
# Plot sihouette width (higher is better)
plot(1:10, sil_width,
xlab = "Number of clusters",
ylab = "Silhouette Width")
lines(1:10, sil_width)
plot(1:10, sil_width,
xlab = "Number of clusters",
ylab = "Silhouette Width")
pam_fit <- pam(gower_dist, diss = TRUE, k = 3)
pam_results <- college_clean %>%
dplyr::select(-name) %>%
mutate(cluster = pam_fit$clustering) %>%
group_by(cluster) %>%
do(the_summary = summary(.))
pam_results$the_summary
college_clean[pam_fit$medoids, ]
tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)
tsne_data <- tsne_obj$Y %>%
data.frame() %>%
setNames(c("X", "Y")) %>%
mutate(cluster = factor(pam_fit$clustering),
name = college_clean$name)
ggplot(aes(x = X, y = Y), data = tsne_data) +
geom_point(aes(color = cluster))
tsne_data %>%
filter(X > 15 & X < 25,
Y > -15 & Y < -10) %>%
left_join(college_clean, by = "name") %>%
collect %>%
.[["name"]]
rm(list=ls())
gc()
getwd()
setwd("../Desktop/Kaggle & Coursera/Kaggle Projects/March Machine Learning Mania 2017/2017 Prediction/")
Team = read.csv("Raw Data/Teams.csv")
Pred_Tournament_Top = read.csv("Pred_Tournament_Top_xgb_Elo_0313.csv")
Pred_Tournament_Top_Detail = Pred_Tournament_Top
Pred_Tournament_Top_Detail$Team1 = substr(Pred_Tournament_Top_Detail$id,6,9)
Pred_Tournament_Top_Detail$Team2 = substr(Pred_Tournament_Top_Detail$id,11,14)
Detail = merge(Pred_Tournament_Top_Detail,Team,by.x="Team1",by.y="Team_Id",all.x=T)
names(Detail)[5] = "Team1_Name"
Detail = merge(Detail,Team,by.x="Team2",by.y="Team_Id",all.x=T)
names(Detail)[6] = "Team2_Name"
Detail = Detail[,c(3,2,5,1,6,4)]
View(Detail)
Detail[Detail$Team1_Name=="UCLA" | Detail$Team2_Name=="UCLA",]
Detail[Detail$Team1_Name=="Miami FL" | Detail$Team2_Name=="Miami FL",]
Detail[Detail$Team1_Name=="Baylor" | Detail$Team2_Name=="Baylor",]
Detail[Detail$Team1_Name=="North Carolina" | Detail$Team2_Name=="North Carolina",]
Detail[Detail$Team1_Name=="Villanova" | Detail$Team2_Name=="Villanova",]
Detail[Detail$Team1_Name=="Xavier" | Detail$Team2_Name=="Xavier",]
Detail[Detail$Team1_Name=="Florida" | Detail$Team2_Name=="Florida",]
?fwrite
?gsub
x <- "<i>the text I need to extract</i></b></a></div>"
gsub(".*<i>|</i>.*", "", x)
rm(list=ls())
gc()
getwd()
setwd("../Desktop/Kaggle & Coursera/Kaggle Projects/March Machine Learning Mania 2017/2017 Prediction/")
Team = read.csv("Raw Data/Teams.csv")
Pred_Tournament_Top = read.csv("Pred_Tournament_Top_xgb_Elo_0313.csv")
Pred_Tournament_Top_Detail = Pred_Tournament_Top
Pred_Tournament_Top_Detail$Team1 = substr(Pred_Tournament_Top_Detail$id,6,9)
Pred_Tournament_Top_Detail$Team2 = substr(Pred_Tournament_Top_Detail$id,11,14)
Detail = merge(Pred_Tournament_Top_Detail,Team,by.x="Team1",by.y="Team_Id",all.x=T)
names(Detail)[5] = "Team1_Name"
Detail = merge(Detail,Team,by.x="Team2",by.y="Team_Id",all.x=T)
names(Detail)[6] = "Team2_Name"
Detail = Detail[,c(3,2,5,1,6,4)]
Detail[Detail$Team1_Name=="Florida" | Detail$Team2_Name=="Florida",]
Detail[Detail$Team1_Name=="North Carolina" | Detail$Team2_Name=="North Carolina",]
library(tm)
installed.packages("tm")
library(tm)
install.packages("tm")
library(tm)
library(tm)
library(tm)
installed.packages("slam")
install.packages("slam")
library(tm)
library(tm)
install.packages('devtools')
library(devtools)
library(tm)
installed.packages("slam")
install.packages("slam")
library(tm)
290*4
180.12/4
1345.12-290
1345.12-290*4
185.12/4
